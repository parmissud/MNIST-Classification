# -*- coding: utf-8 -*-
"""3-MNIST_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x2lbbLxvK51Ud4rI1g7ZieVe6FKhVOVU

<center> <b>Machine Learning - SBU FALL 2024</b></center>
"""

student_number = '400243083'
Name = 'Parmiss'
Last_Name = 'Yousefi'

"""# Imported Libraries"""

import torch
import torch.nn as nn               # For building neural network layers
import torch.optim as optim         # For optimizers (SGD, Adam, etc.)
import torch.nn.functional as F     # For activation functions and other operations
from torchvision import datasets, transforms  # For MNIST dataset and data preprocessing if its needed
from torch.utils.data import DataLoader       # For batching and shuffling data

# Additional libraries for visualization and analysis
import matplotlib.pyplot as plt     # For plotting graphs and visualizations
import numpy as np                  # For numerical operations
import seaborn as sns               # For plotting the confusion matrix heatmap
from sklearn.metrics import confusion_matrix  # For computing the confusion matrix

"""# Build a Model for the MNIST Dataset

In this Notebook, your goal is to build a machine learning model that achieves an accuracy of **98% or higher** on the MNIST dataset. The MNIST dataset contains images of handwritten digits (0-9), and it’s a popular benchmark for evaluating classification models. To achieve this accuracy threshold, you’ll need to carefully design, train, and evaluate your model.

## Objective

Using only the provided libraries, you will:
1. **Load and preprocess** the MNIST data.
2. **Build** a neural network model suitable for image classification.
3. **Train** the model on the training data, monitoring accuracy and adjusting parameters as needed.
4. **Evaluate** the model on the test data to ensure it meets the required accuracy threshold.

Each cell in this section will guide you through these steps, with descriptions explaining the purpose of each step and what you need to do. Follow the instructions carefully, and use your understanding of neural networks to achieve the best possible performance on the MNIST dataset.

## Step 1: Loading and Preprocessing the MNIST Dataset

In this cell, you will:
1. **Load the MNIST dataset**: This dataset consists of 28x28 grayscale images of handwritten digits (0–9). Each image is represented as a 28x28 array of pixel values, and each label corresponds to the digit in the image.
   
2. **Define key variables**:
   - `num_classes`: The number of unique classes in the dataset (0–9).
   - `input_shape`: The shape of each image, which will help in defining the model's input layer.
   - `num_features`: The total number of pixels in each image, useful if you need to flatten the images into a 1D vector for the model.

3. **Normalize the pixel values**: The pixel values are divided by 255 to rescale them from their original range of 0–255 to a range of 0–1. This normalization step helps the model converge faster and improves performance.

4. **One-hot encode the labels**: Convert the labels into a one-hot encoded format. Instead of representing the labels as single integers (e.g., `3` for the digit 3), one-hot encoding represents each label as a binary vector (e.g., `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]` for the digit 3). This format is commonly used for multi-class classification problems and is required for training neural networks effectively.

By the end of this cell, your data will be ready for building and training the model.
"""

from tensorflow.keras.datasets import mnist
# Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Define key variables
num_classes = len(np.unique(y_train))  # Number of unique digits (0-9)
input_shape = x_train.shape[1:]        # Shape of each image (28, 28)
num_features = np.prod(input_shape)    # Total number of pixels per image (28*28)

# Normalize the pixel values to the range [0, 1]
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

# One-hot encode the labels manually
def one_hot_encode(labels, num_classes):
    # Create a zero matrix of shape (number of labels, number of classes)
    one_hot = np.zeros((labels.shape[0], num_classes))
    # Set the appropriate index to 1 for each label
    one_hot[np.arange(labels.shape[0]), labels] = 1
    return one_hot

# Apply the custom one-hot encoding
y_train = one_hot_encode(y_train, num_classes)
y_test = one_hot_encode(y_test, num_classes)

# Verify the one-hot encoding
print(f"First label (one-hot encoded): {y_train[0]}")
print(f"Shape of one-hot encoded labels: {y_train.shape}")


# Print some dataset details for verification
print(f"Training data shape: {x_train.shape}")
print(f"Test data shape: {x_test.shape}")
print(f"Number of classes: {num_classes}")
print(f"Input shape: {input_shape}")

"""## Step 2: Building and Optimizing Your Model for MNIST

Your goal in this cell is to build a neural network model that achieves an accuracy of **98% or higher** on the MNIST dataset. To achieve this, you will need to:
1. **Define the model architecture**: You can experiment with the number of layers, units in each layer, and other architectural details to optimize your model’s performance.
2. **Choose an optimizer**: Test different optimizers (SGD, Adagrad, RMSprop, Adam) and fine-tune their hyperparameters (such as learning rate, momentum, etc.).
3. **Search for optimal hyperparameters**: You can use hyperparameter tuning methods such as **Hyperband**, **Random Search**, or **Bayesian Optimization** to find the best hyperparameter values efficiently.

### Available Tuning Options

- **Hyperband**: Efficiently finds the best hyperparameters by exploring a wide range and narrowing down based on performance.
- **Random Search**: Tries a random combination of hyperparameters within defined ranges.
- **Bayesian Optimization**: Uses a probabilistic approach to find the best parameters based on past evaluations.

### Final Task

Once you’ve achieved a model with over **98% accuracy** on the MNIST test set:
1. **Print the optimal hyperparameters** for your best-performing model.
2. **Build and compile the model** using these optimal parameters.
3. **Display the model summary** at the end of the cell.

By following this approach, you will gain insights into how different optimizers and model configurations impact performance, ultimately helping you build an effective and efficient model for the MNIST dataset.


### if you achieve the threshold before testing all the items above you must continue and interpert the result and the change effect!

### Bayesian Optimization
"""


import optuna
# Define the fully connected neural network
class FullyConnectedNN(nn.Module):
    def __init__(self, input_size=28 * 28, num_layers=2, units_per_layer=[128, 128], dropout_rate=0.2):
        super(FullyConnectedNN, self).__init__()
        layers = []
        in_features = input_size

        # Create hidden layers
        for units in units_per_layer:
            layers.append(nn.Linear(in_features, units))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout_rate))
            in_features = units

        # Output layer
        layers.append(nn.Linear(in_features, 10))  # MNIST has 10 output classes

        self.model = nn.Sequential(*layers)

    def forward(self, x):
        x = x.view(x.size(0), -1)  # Flatten the input
        return self.model(x)

# Define the objective function for Optuna
def objective(trial):
    # Hyperparameters
    num_layers = trial.suggest_int('num_layers', 2, 4)  # Number of layers
    units_per_layer = [trial.suggest_int(f'units_layer_{i}', 128, 512, step=64) for i in range(num_layers)]
    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.3)  # Dropout for regularization
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)  # Learning rate
    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])  # Batch size
    optimizer_name = trial.suggest_categorical('optimizer', ['SGD', 'Adagrad', 'RMSprop', 'Adam'])

    # Data preprocessing
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
    train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
    test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # Create the model
    model = FullyConnectedNN(num_layers=num_layers, units_per_layer=units_per_layer, dropout_rate=dropout_rate)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    # Select optimizer
    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=learning_rate)
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(5):  # Shorter epochs for faster trials
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

    # Evaluate the model
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = correct / total
    return accuracy

# Run Optuna study
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=30)

# Display the best hyperparameters and accuracy
print(f"Best hyperparameters: {study.best_params}")
print(f"Best test accuracy: {study.best_value:.4f}")

"""### Hyperband"""

from sklearn.model_selection import train_test_split

# Split the training data into training and validation subsets
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)

# Define the model creation function using PyTorch
class MNISTModel(nn.Module):
    def __init__(self, num_layers=2, units_per_layer=[128, 128], dropout_rate=0.2):
        super(MNISTModel, self).__init__()

        layers = []
        input_size = 28 * 28  # Flattened image size (28x28)

        # Add a flatten layer at the beginning
        self.flatten = nn.Flatten()

        # Adding the hidden layers
        for i in range(num_layers):
            layers.append(nn.Linear(input_size, units_per_layer[i]))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout_rate))
            input_size = units_per_layer[i]  # Update input size for the next layer

        layers.append(nn.Linear(input_size, 10))  # 10 classes for MNIST digits
        layers.append(nn.Softmax(dim=1))  # Softmax for multi-class classification

        self.model = nn.Sequential(*layers)

    def forward(self, x):
        x = self.flatten(x)  # Flatten the input image
        return self.model(x)

# Define the objective function for Optuna
def objective(trial):
    # Hyperparameters to tune
    num_layers = trial.suggest_int('num_layers', 2, 3)  # Number of layers: 2 or 3
    units_per_layer = [trial.suggest_int(f'units_layer_{i}', 128, 1024) for i in range(num_layers)]  # Units in each layer
    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)  # Dropout rate: between 0.1 and 0.5
    optimizer_type = trial.suggest_categorical('optimizer_type', ['adam', 'sgd', 'rmsprop'])  # Optimizer type
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)  # Learning rate using log scale

    # Create model with current hyperparameters
    model = MNISTModel(num_layers=num_layers, units_per_layer=units_per_layer, dropout_rate=dropout_rate)

    # Check if GPU is available and move model to the appropriate device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Choose optimizer
    if optimizer_type == "adam":
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_type == "sgd":
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    elif optimizer_type == "rmsprop":
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)

    criterion = nn.CrossEntropyLoss()

    # Data loaders (using x_train and y_train directly)
    train_loader = DataLoader(list(zip(x_train, y_train)), batch_size=128, shuffle=True)
    val_loader = DataLoader(list(zip(x_val, y_val)), batch_size=128, shuffle=False)

    # Training loop
    model.train()
    for epoch in range(5):  # Keep the number of epochs small for quick evaluation
        for inputs, labels in train_loader:
            # Move data to the GPU
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels.argmax(dim=1))  # CrossEntropyLoss expects indices
            loss.backward()
            optimizer.step()

    # Evaluate the model on the validation set
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            # Move data to the GPU
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels.argmax(dim=1)).sum().item()

    val_accuracy = correct / total
    return val_accuracy  # Return the accuracy to be maximized by Optuna

# Use Optuna's Hyperband for optimization
study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(), pruner=HyperbandPruner())  # Hyperband Pruner
study.optimize(objective, n_trials=7)  # Run 7 trials for optimization

# Print the best hyperparameters and accuracy found
print(f"Best hyperparameters: {study.best_params}")
print(f"Best validation accuracy: {study.best_value:.4f}")

from sklearn.model_selection import train_test_split

# Split the training data into training and validation subsets
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)

# Define the model creation function using PyTorch
class MNISTModel(nn.Module):
    def __init__(self, num_layers=2, units_per_layer=[128, 128], dropout_rate=0.2):
        super(MNISTModel, self).__init__()

        layers = []
        input_size = 28 * 28  # Flattened image size (28x28)

        # Add a flatten layer at the beginning
        self.flatten = nn.Flatten()

        # Adding the hidden layers
        for i in range(num_layers):
            layers.append(nn.Linear(input_size, units_per_layer[i]))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout_rate))
            input_size = units_per_layer[i]  # Update input size for the next layer

        layers.append(nn.Linear(input_size, 10))  # 10 classes for MNIST digits
        layers.append(nn.Softmax(dim=1))  # Softmax for multi-class classification

        self.model = nn.Sequential(*layers)

    def forward(self, x):
        x = self.flatten(x)  # Flatten the input image
        return self.model(x)

# Define the objective function for Optuna
def objective(trial):
    # Hyperparameters to tune
    num_layers = trial.suggest_int('num_layers', 2, 3)  # Number of layers: 2 or 3
    units_per_layer = [trial.suggest_int(f'units_layer_{i}', 128, 1024) for i in range(num_layers)]  # Units in each layer
    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)  # Dropout rate: between 0.1 and 0.5
    optimizer_type = trial.suggest_categorical('optimizer_type', ['adam', 'sgd', 'rmsprop'])  # Optimizer type
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)  # Learning rate using log scale

    # Create model with current hyperparameters
    model = MNISTModel(num_layers=num_layers, units_per_layer=units_per_layer, dropout_rate=dropout_rate)

    # Check if GPU is available and move model to the appropriate device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # Choose optimizer
    if optimizer_type == "adam":
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_type == "sgd":
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    elif optimizer_type == "rmsprop":
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)

    criterion = nn.CrossEntropyLoss()

    # Data loaders (using x_train and y_train directly)
    train_loader = DataLoader(list(zip(x_train, y_train)), batch_size=128, shuffle=True)
    val_loader = DataLoader(list(zip(x_val, y_val)), batch_size=128, shuffle=False)

    # Training loop
    model.train()
    for epoch in range(10):  # Keep the number of epochs small for quick evaluation
        for inputs, labels in train_loader:
            # Move data to the GPU
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels.argmax(dim=1))  # CrossEntropyLoss expects indices
            loss.backward()
            optimizer.step()

    # Evaluate the model on the validation set
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            # Move data to the GPU
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels.argmax(dim=1)).sum().item()

    val_accuracy = correct / total
    return val_accuracy  # Return the accuracy to be maximized by Optuna

# Use Optuna's Hyperband for optimization
study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(), pruner=HyperbandPruner())  # Hyperband Pruner
study.optimize(objective, n_trials=40)  # Run 7 trials for optimization

# Print the best hyperparameters and accuracy found
print(f"Best hyperparameters: {study.best_params}")
print(f"Best validation accuracy: {study.best_value:.4f}")

"""### compiling the model and summary"""

import torch
import torch.nn as nn
import torch.optim as optim

# Define the optimal parameters
optimal_params = {
    'num_layers': 3,
    'units_per_layer': [512, 448, 384],
    'dropout_rate': 0.13511168748625976,
    'learning_rate': 0.00010514586427242404,
    'optimizer': 'Adam'
}

# Initialize the model
model = FullyConnectedNN(
    input_size=28 * 28,
    units_per_layer=optimal_params['units_per_layer'],
    dropout_rate=optimal_params['dropout_rate']
)

# Move the model to the appropriate device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Define the optimizer (compilation step in PyTorch)
optimizer = getattr(optim, optimal_params['optimizer'])(model.parameters(), lr=optimal_params['learning_rate'])

# Print the model summary
print(model)

"""## Step 3: Training, Evaluating, and Saving Your Model

In this part, you will:
1. **Train the Model**: Train your model on the MNIST training data for a specified number of epochs. The `fit` function will show the training and validation accuracy/loss at each epoch, allowing you to monitor the model’s progress.
   
2. **Make Predictions**: Once training is complete, generate predictions on the test data. While not required for accuracy calculation, this can be useful if you want to further analyze the model’s predictions.

3. **Evaluate the Model**: Use the `evaluate` function to calculate the model's loss and accuracy on the test set. This will give you an objective measure of the model’s performance. Remember, the goal is to achieve **98% or higher** accuracy.

4. **Save the Model**: After achieving satisfactory accuracy, save your model to disk for later use. This allows you to load the trained model in the future without retraining.

### Important Notes:
- **Training Parameters**: You can adjust the number of epochs or batch size in the `fit` function if needed to reach the required accuracy.
- **Model Storage**: The saved model file (`mnist_model.keras`) will store the entire model structure and weights, enabling you to load and use it later without needing to rebuild or retrain.

By completing these steps, you will have a fully trained and saved model capable of achieving high accuracy on the MNIST dataset.
"""

class FullyConnectedNN(nn.Module):
    def __init__(self, input_size=28 * 28, num_layers=2, units_per_layer=[128, 128], dropout_rate=0.2):
        super(FullyConnectedNN, self).__init__()
        layers = []
        in_features = input_size

        # Create hidden layers
        for units in units_per_layer:
            layers.append(nn.Linear(in_features, units))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout_rate))
            in_features = units

        # Output layer
        layers.append(nn.Linear(in_features, 10))  # MNIST has 10 output classes

        self.model = nn.Sequential(*layers)

    def forward(self, x):
        x = x.view(x.size(0), -1)  # Flatten the input
        return self.model(x)

    def fit(self, train_data, val_data, epochs=20, batch_size=32, learning_rate=0.001, optimizer='Adam'):
        criterion = nn.CrossEntropyLoss()
        # Define optimizer
        optimizer = getattr(optim, optimizer)(self.parameters(), lr=learning_rate)

        train_accuracies = []
        val_accuracies = []
        train_losses = []
        val_losses = []

        for epoch in range(epochs):
            # Training phase
            self.train()
            running_loss = 0.0
            correct = 0
            total = 0
            for i in range(0, len(train_data[0]), batch_size):
                inputs = train_data[0][i:i+batch_size]
                labels = train_data[1][i:i+batch_size]

                labels = torch.argmax(labels, dim=1)

                # Move data to device
                inputs, labels = inputs.view(inputs.size(0), -1).to(device), labels.to(device)

                optimizer.zero_grad()

                # Forward pass
                outputs = self(inputs)
                loss = criterion(outputs, labels)

                # Backward pass
                loss.backward()
                optimizer.step()

                running_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

            train_accuracy = correct / total
            train_loss = running_loss / (len(train_data[0]) // batch_size)

            train_accuracies.append(train_accuracy)
            train_losses.append(train_loss)

            # Validation phase
            self.eval()
            val_correct = 0
            val_total = 0
            val_loss = 0.0
            with torch.no_grad():
                for i in range(0, len(val_data[0]), batch_size):
                    inputs = val_data[0][i:i+batch_size]
                    labels = val_data[1][i:i+batch_size]
                    labels = torch.argmax(labels, dim=1)

                    inputs, labels = inputs.view(inputs.size(0), -1).to(device), labels.to(device)

                    outputs = self(inputs)
                    loss = criterion(outputs, labels)
                    val_loss += loss.item()

                    _, predicted = torch.max(outputs, 1)
                    val_total += labels.size(0)
                    val_correct += (predicted == labels).sum().item()

            val_accuracy = val_correct / val_total
            val_loss = val_loss / (len(val_data[0]) // batch_size)

            val_accuracies.append(val_accuracy)
            val_losses.append(val_loss)

            print(f"Epoch [{epoch + 1}/{epochs}], "
                  f"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, "
                  f"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}")

        return {
            'train_accuracies': train_accuracies,
            'val_accuracies': val_accuracies,
            'train_losses': train_losses,
            'val_losses': val_losses
        }

from sklearn.model_selection import train_test_split

# Assuming x_train and y_train are already defined
# Split x_train and y_train into training and validation sets (80% train, 20% validation)
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)

# Convert to PyTorch tensors
train_data = (torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))
val_data = (torch.tensor(x_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))

# Initialize model and device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = FullyConnectedNN(input_size=28 * 28, units_per_layer=[512, 448, 384], dropout_rate=0.135).to(device)

# Prepare the data (assuming you have your train and validation data in x_train, y_train, x_val, y_val)

train_data = (torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))
val_data = (torch.tensor(x_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))

# Call fit method
history = model.fit(train_data, val_data, epochs=20, batch_size=32, learning_rate=0.0001)

import torch
import numpy as np
from sklearn.metrics import confusion_matrix

# Initialize variables for true and predicted labels
true_labels = []
predicted_labels = []
incorrect_indices = []

# Set model to evaluation mode
model.eval()

correct = 0
total = 0

with torch.no_grad():
    for i in range(0, len(x_test_tensor), batch_size):
        # Create batches manually
        inputs = x_test_tensor[i:i+batch_size]
        labels = y_test_tensor[i:i+batch_size]

        # Convert one-hot encoded labels to class indices (if applicable)
        labels = torch.argmax(labels, dim=1)

        # Move data to the GPU/CPU
        inputs, labels = inputs.view(inputs.size(0), -1).to(device), labels.to(device)

        # Forward pass
        outputs = model(inputs)
        _, predicted = torch.max(outputs, 1)

        # Store true and predicted labels for confusion matrix
        true_labels.extend(labels.cpu().numpy())
        predicted_labels.extend(predicted.cpu().numpy())

        # Calculate accuracy
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        incorrect_indices.extend(np.where(predicted.cpu().numpy() != labels.cpu().numpy())[0] + i)


# Compute the test accuracy
test_accuracy = correct / total
print(f"Test Accuracy: {test_accuracy:.4f}")

# Convert your data into PyTorch tensors
x_train_tensor = torch.tensor(x_train, dtype=torch.float32)  # Ensure input is float32
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)  # Keep as float32 for one-hot encoded data
x_test_tensor = torch.tensor(x_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

# Training the model
epochs = 20
batch_size = 32
criterion = nn.CrossEntropyLoss()
model.train()

for epoch in range(epochs):
    running_loss = 0.0
    correct = 0
    total = 0

    for i in range(0, len(x_train_tensor), batch_size):
        # Create batches manually
        inputs = x_train_tensor[i:i+batch_size]
        labels = y_train_tensor[i:i+batch_size]

        # Convert one-hot encoded labels to class indices
        labels = torch.argmax(labels, dim=1)

        # Move data to the GPU/CPU
        inputs, labels = inputs.view(inputs.size(0), -1).to(device), labels.to(device)

        # Forward pass
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        # Calculate training accuracy
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    epoch_accuracy = correct / total
    print(f"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / (len(x_train_tensor) // batch_size):.4f}, Accuracy: {epoch_accuracy:.4f}")

# Evaluating the model
model.eval()
correct = 0
total = 0

with torch.no_grad():
    for i in range(0, len(x_test_tensor), batch_size):
        # Create batches manually
        inputs = x_test_tensor[i:i+batch_size]
        labels = y_test_tensor[i:i+batch_size]

        # Convert one-hot encoded labels to class indices
        labels = torch.argmax(labels, dim=1)

        # Move data to the GPU/CPU
        inputs, labels = inputs.view(inputs.size(0), -1).to(device), labels.to(device)

        # Forward pass
        outputs = model(inputs)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()


test_accuracy = correct / total
print(f"Test Accuracy: {test_accuracy:.4f}")

"""### saving the model .keras"""



# Dummy input for exporting
dummy_input = torch.randn(1, 1, 28, 28)
dummy_input = dummy_input.to(device)
# Export the model to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "fully_connected_nn.onnx",
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
)
print("Model saved as 'fully_connected_nn.onnx'")

import tensorflow as tf

# Load the TensorFlow model
model = tf.saved_model.load('/content/fully_connected_nn.onnx')

# Convert to Keras model
keras_model = tf.keras.models.load_model('/content/fully_connected_nn.onnx')

# Save the Keras model in .keras format
model.save('fully_connected_nn.keras')

"""## Step 4: Visualizing Model Performance

In this final step, you will visualize your model's performance using a series of plots. These visualizations will help you understand how well the model performed, identify areas for potential improvement, and interpret any misclassifications.

1. **Plot the Training History**: This plot shows the training and validation accuracy over each epoch. By examining this plot, you can see how the model's accuracy improved with each epoch and check if there’s any overfitting (e.g., if training accuracy is much higher than validation accuracy).

2. **Plot the Confusion Matrix**: The confusion matrix provides a detailed view of the model's performance across each digit class (0–9). Each cell in the matrix shows the number of true vs. predicted classifications for each class. This helps you identify specific digits that the model struggles to classify correctly, as they may have higher misclassification counts.

3. **Visualize Misclassified Images**: In this grid, you’ll see some of the images that were incorrectly classified by the model. Each image shows the true label and the predicted label. Examining these misclassified examples can give you insight into where the model might have difficulty, such as digits that look similar (e.g., 3 and 5). It can also help you decide if additional data processing or model adjustments are needed.

### Tips for Interpretation:
- **Training History**: If the validation accuracy plateaus early or diverges significantly from training accuracy, consider revisiting your model architecture or hyperparameters.
- **Confusion Matrix**: Look for off-diagonal cells with high values, as these indicate common misclassifications.
- **Misclassified Images**: Understanding these cases can guide you in tuning or improving your model, such as adding more data for challenging classes.

By analyzing these plots, you’ll gain a deeper understanding of your model’s strengths and weaknesses on the MNIST dataset.
"""

# Plot the training and validation accuracy
plt.figure(figsize=(10, 6))
plt.plot(history['train_accuracies'], label='Train Accuracy')
plt.plot(history['val_accuracies'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.grid(True)
plt.show()

cm = confusion_matrix(true_labels, predicted_labels)

# Plot the confusion matrix heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

# Visualize the first 20 incorrectly classified images
plt.figure(figsize=(10, 8))
for i, idx in enumerate(incorrect_indices[:20]):
    plt.subplot(4, 5, i + 1)
    plt.imshow(x_test[idx], cmap='gray')
    plt.title(f"True: {np.argmax(y_test[idx])} - Pred: {predicted_labels[idx]}")
    plt.axis('off')

plt.tight_layout()
plt.show()